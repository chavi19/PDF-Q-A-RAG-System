RAG (Retrieval-Augmented Generation) – Detailed Notes

This document explains RAG concepts, backend architecture,
and how they are implemented in this project.

1. Why RAG is needed
LLMs (like GPT, Claude, etc) have limitations:
•	They are trained on static data (not real-time)
•	They cannot access private or personal data
•	They have a context length limit
•	They may hallucinate when unsure
To overcome this, we attach an external knowledge source to the LLM.
This approach is called RAG (Retrieval-Augmented Generation).
RAG allows the model to:
•	Answer questions using private documents
•	Stay up-to-date
•	Reduce hallucinations
•	Handle large documents beyond context limits
________________________________________
2. What is RAG?
RAG = Information Retrieval + Language Generation
In RAG:
1.	Relevant information is retrieved from an external knowledge base
2.	That information is provided as context to the LLM
3.	The LLM generates a grounded and accurate response
________________________________________
3. Core Components of RAG
RAG has 4 main components:
1.	Document Loader
2.	Text Splitter
3.	Vector Store
4.	Retriever
In frameworks like LangChain, these are provided as abstractions.
In our project, we implement RAG from scratch, without LangChain.
________________________________________
4. Document Loader
Concept
Document loaders fetch data from various sources such as:
•	PDFs
•	Text files
•	Databases
•	Cloud storage (S3, etc.)
In LangChain, data is converted into a standard Document object:
Document(
  page_content="text",
  metadata={"source": "filename.pdf"}
)
Concept 	    LangChain	            Your project
Loader	        PyPDFLoader	               load_pdf()
Output	        Document objects	        List[str]
Metadata	    Automatic	            Minimal (active_pdf)
________________________________________
5. Text Splitting
Why Text Splitting is Needed
•	LLMs have context length limitations
•	Large documents cannot be processed at once
•	Smaller chunks improve:
        Embedding quality
    	Semantic search accuracy
    	Retrieval performance
________________________________________
Types of Text Splitters
1.	Length-based
	- Fixed-size chunks (e.g., 800 characters)
	- May break sentences
2.	Text-structure-based
	- Splits by paragraphs, sentences, lines
	- More natural chunk boundaries
3.	Document-structure-based
	- Designed for structured data like:	Code (Python, JS), HTML, Markdown
4.	Semantic-based
	- Splits based on topic/meaning
	- More advanced and computationally expensive
________________________________________
In Our Project: Splitiing texts manually, not using RecusriveTextSplitter using LangChain
•	We use length-based character splitting with overlap
•	Example:
	Chunk size: 800 characters
	Overlap: 200 characters
•	Simple, predictable, and efficient
________________________________________
6. Embeddings
What are Embeddings?
•	Embeddings convert text into numerical vectors
•	Vectors capture semantic meaning
•	Similar meanings → closer vectors
Example:
"IPL match"  → [0.21, -0.88, 0.45, ...]
"Cricket game" → very similar vector
Why Embeddings?
•	Enables semantic search
•	Better than keyword-based search
•	Required for similarity comparison
In Our Project
•	We generate embeddings for:
	- PDF text chunks
	- User questions
•	Both lie in the same vector space
________________________________________
7. Vector Store
What is a Vector Store?
A vector store is a system designed to:
•	Store embedding vectors
•	Perform similarity search
•	Maintain fast retrieval at scale
Most likely using:
•	Cosine similarity or
•	Dot product
This line is the heart of RAG:
•	results = vector_store.search(query_embedding, top_k=3)
•	This is equivalent to:	similarity_search(query, k=3)
Features
•	Vector storage (RAM / disk)
•	Similarity search (cosine / dot product)
•	Indexing for fast lookup
•	Optional metadata support
________________________________________
Vector Store vs Vector Database
Vector Store	Vector Database
Storage + retrieval	Storage + retrieval
No distribution	Distributed
No auth	Auth, backup, scaling
Example: FAISS	Example: Pinecone
ChromaDB sits between both.
In Our Project
•	We use a local in-memory vector store
•	Focused on:	Storage & Similarity search
•	No distributed features (acceptable for demo)
________________________________________
8. Retriever 
Retriever is a real-time process that finds the most relevant information from a pre-built index based on the user’s query.
________________________________________
Retriever Steps (Detailed)
1.	Query Embedding: User question is converted into an embedding vector
2.	Semantic Search
	- Query vector is compared with stored vectors
	- Uses similarity search
3.	Ranking: Retrieved vectors are ranked based on similarity score
4.	Context Selection
	- Top-K relevant text chunks are fetched
	- These chunks form the context
________________________________________
In Our Project
•	We use a basic vector store retriever
•	Top-K semantic similarity search
•	No advanced retrievers like: MMR, Multi-query, Wikipedia retriever
________________________________________
9. Augmentation
What is Augmentation?
Augmentation means:
•	Combining user query + retrieved context
•	Creating a new prompt for the LLM
Example prompt:
You are a helpful assistant.
Answer only using the provided context.
If context is insufficient, say "I don't know".

Context:
<retrieved chunks>

Question:
<user query>

Purpose
•	Prevent hallucinations
•	Ground responses in real data
________________________________________
10. Generation
What Happens in Generation?
•	LLM receives:
	- User question
	- Retrieved and augmented context
•	LLM uses:
	- Its parametric knowledge (trained weights)
	- Your external non-parametric knowledge
•	Generates final response
________________________________________
11. Parametric vs Non-Parametric Knowledge: The LLM provides parametric knowledge, while the document embeddings act as non-parametric memory
Type	            Source
Parametric	    LLM training data
Non-parametric	Vector store (PDF data)

RAG combines both → this is its strength.
________________________________________
12. 4 Steps of RAG Pipeline
1. Indexing
Preparing the knowledge base
Sub-steps:
•	Document ingestion
•	Text chunking
•	Embedding generation
•	Storage in vector store
chunks = load_pdf(...)
embeddings = embedding_model.embed_texts(chunks)
vector_store.add(embeddings, chunks)
2. Retrieval
•	Convert query to embedding
•	Semantic similarity search
•	Rank results
•	Fetch top-K chunks
query_embedding = embedding_model.embed_query(question)
results = vector_store.search(query_embedding, top_k=3)
3. Augmentation
•	Combine query + retrieved context
•	Build LLM prompt
answer = qa_engine.generate_answer(question, results)
4. Generation
•	LLM generates grounded answer
________________________________________
13. Problems RAG Solves
RAG solves three major problems:
1.	Private data access
2.	Recent / dynamic information
3.	Hallucination reduction

_______________________________________
FastAPI : Is a modern, fast, web framework for building APIs with Python. Automatic, interactive API documentation. You get Swagger UI. It's asynchronous

________________________________________
14. Mapping RAG to Our Project 
Our project implements a full RAG pipeline from scratch using FastAPI, where PDFs are indexed into a vector store, relevant chunks are retrieved using semantic search, and an LLM generates accurate, context-grounded answers.
 
Steps:- 
1.	Created repo 
2.	python -m venv venv(python virtual environment)
3.	You must activate the virtual environment before installing anything.-> venv\Scripts\activate
4.	Created requirements.txt file in that added all the things to be downloaded & run-> pip install -r requirements.txt 
5.	Created gitignore 
6.	.env is where u keep ur secret keys or ids like OpenAI API Key(this doesn’t push to GitHub so it’s safe)
7.	File pdf_loader.py & data/uploaded_pdfs/
This file handles safe PDF text extraction and chunking using PyMuPDF. It converts PDFs into clean, overlapping text chunks while enforcing strict memory and safety limits to make the RAG pipeline stable and accurate.
8.	Search abt LLM model on huggingface(https://huggingface.co/google/flan-t5-base)
9.	Created test files test_pdf_loader.py to check
10.	Without RAG → LLM guesses ❌
    With RAG → LLM answers from your PDFs only
11.	I chose an open-source model to avoid API dependency and cost
12.	Yes, the backend exposes REST APIs using FastAPI and the frontend is built with Streamlit for rapid interaction. The focus of the project is the RAG pipeline rather than UI complexity
13.	app/ ── main.py       ← FastAPI entry point     &    ── routes.py     ← API endpoints
14.	Backend: python -m uvicorn app.main:app –reload
15.	Frontend : streamlit run app.py
16.	How is your backend structured? => We separated responsibilities into services for PDF loading, chunking, embeddings, vector storage, and QA. FastAPI routes orchestrate these components into a clean RAG pipeline.
17.	Global objects (core backend memory)
embedding_model = EmbeddingModel()
qa_engine = QAEngine()
vector_store = None
active_pdf = None
18.	Why these are global
Loaded once when the server starts
Reused across requests
Saves time and memory
This is called application-level state.
“We keep the embedding model and vector store in memory to avoid rebuilding them for every request.”
•  POST because we are sending data
•  UploadFile is FastAPI’s optimized file handler
•  Async allows the server to handle multiple requests efficiently during I/O operations like file upload.

19.	95% accuracy? Since this is a RAG-based system, accuracy is measured in terms of correct document retrieval and answer grounding.
I evaluated the system using real policy questions and found that around 95% of the queries retrieved the correct clause and generated factually accurate answers based on the document context.
The remaining cases were either ambiguous or out of scope, which the system handled safely without hallucination.
1>	High-quality embeddings (biggest contributor)
You use: all-MiniLM-L6-v2
•	Understands meaning, not keywords => Contribution to accuracy: ~40%
2️ Chunking strategy (prevents context dilution)
You split PDFs into:
•	Small overlapping chunks, Answers that cross paragraph boundaries are still captured, No broken sentences => Contribution: ~25%
3️ FAISS semantic search (fast + exact)
•	Computes vector distance & Returns top-K closest semantic matches
You only pass top relevant chunks to LLM → less noise => Contribution: ~20%
4️ LLM is forced to stay grounded
Your prompt:
Answer strictly from the given context.
This prevents hallucination. => Contribution: ~10%
5️ Manual evaluation (how you measured 95%)

1. "Why use RAG instead of fine-tuning?"
Answer:
•	RAG is dynamic (update docs without retraining)
•	Lower cost (no GPU training needed)
•	More transparent (shows source chunks)
•	Better for factual accuracy
2. "How do you handle large PDFs?"
Answer:
•	Chunking splits into manageable pieces
•	FAISS handles millions of vectors efficiently
•	Could add pagination or streaming for very large files
3. "What if the answer isn't in the PDF?"
Answer:
•	System checks if chunks are relevant (distance threshold)
•	Returns "No relevant information found" message
•	Could add confidence scoring
4. "How would you scale this?"
Answer:
•	Use production vector DB (Pinecone, Weaviate)
•	Add Redis caching for repeated questions
•	Load balance with multiple API instances
•	Use better LLM (GPT-4 via API)
5. "Security concerns?"
Answer:
•	Add authentication (JWT tokens)
•	Rate limiting to prevent abuse
•	Sanitize file uploads
•	Don't store sensitive data
6. "How accurate is it?"
Answer:
•	Depends on PDF quality and question type
•	Works well for factual retrieval
•	Struggles with complex reasoning
•	Could improve with better LLM (GPT-4)
________________________________________
What Can Be Improved:
1.	Better LLM: Use GPT-4o-mini or Claude
2.	Multi-document support: Query across multiple PDFs
3.	Conversation memory: Follow-up questions
4.	Citation tracking: Show exact source text
5.	Hybrid search: Combine keyword + semantic
6.	Caching: Store common questions
7.	User authentication: Multi-user support
8.	Better chunking: Semantic chunking
9.	Evaluation metrics: Track accuracy
10.	Deployment: Kubernetes, monitoring


Improvements for Future:
1.	Use of Langchain, OpenAI, multiple documents, Better Vector Storage
2.	Advanced retrievers
Current: Top-K similarity search
Improvements:
MMR (Max Marginal Relevance) – avoids redundant chunks
Multi-query retriever – rewrites question into multiple variants
3.	Better text splitting
Current: Fixed-length character-based splitter
Improvements:
Recursive text splitter (paragraph → sentence → character)
Semantic chunking for topic-based splits
4.	User-specific vector stores
Current: Global vector store (shared)
Future: One vector store per user / session => Prevent data leakage between users
5. Technologies: Redis & PostgreSQL + pgvector


