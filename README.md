# PDF Question Answering System with RAG

A Retrieval-Augmented Generation (RAG) based system that allows users to upload PDF documents and ask questions, receiving accurate answers extracted from the document content.

## ğŸ¯ Features

- **PDF Upload & Processing**: Extract and chunk text from PDF documents
- **Semantic Search**: Find relevant content using FAISS vector similarity search
- **AI-Powered Answers**: Generate contextual answers using Flan-T5 language model
- **REST API**: FastAPI backend for scalable deployment
- **Interactive UI**: Clean Streamlit interface for easy interaction

## ğŸ—ï¸ Architecture
```
User Query â†’ Embedding Model â†’ FAISS Search â†’ Retrieved Chunks â†’ LLM â†’ Answer
```

**Tech Stack:**
- **Backend**: FastAPI, Python
- **Vector Store**: FAISS (Facebook AI Similarity Search)
- **Embeddings**: sentence-transformers (all-MiniLM-L6-v2)
- **LLM**: Flan-T5-base (Text-to-Text Generation)
- **PDF Processing**: PyMuPDF
- **Frontend**: Streamlit

## ğŸ“ Project Structure
```
PDF-Q-A-RAG-SYSTEM/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py              # FastAPI application
â”‚   â”œâ”€â”€ routes.py            # API endpoints
â”‚   â”œâ”€â”€ pdf_loader.py        # PDF text extraction
â”‚   â”œâ”€â”€ embeddings.py        # Embedding generation
â”‚   â”œâ”€â”€ vector_store.py      # FAISS vector operations
â”‚   â””â”€â”€ qa_engine.py         # Answer generation
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ app.py               # Streamlit UI
â”œâ”€â”€ data/
â”‚   â””â”€â”€ uploaded_pdfs/       # Temporary PDF storage
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸš€ Installation

### Prerequisites
- Python 3.8+
- pip

### Setup

1. **Clone the repository**
```bash
git clone https://github.com/yourusername/PDF-Q-A-RAG-SYSTEM.git
cd PDF-Q-A-RAG-SYSTEM
```

2. **Install dependencies**
```bash
pip install -r requirements.txt
```

3. **Run the backend**
```bash
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

4. **Run the frontend** (in a new terminal)
```bash
streamlit run frontend/app.py
```

5. **Access the application**
- Frontend: http://localhost:8501
- API Docs: http://localhost:8000/docs

## ğŸ“¦ Docker Deployment
```bash
# Build and run with Docker Compose
docker-compose up --build

# Access at http://localhost:8501
```

## ğŸ”§ How It Works

1. **Document Processing**: PDF is uploaded and text is extracted using PyMuPDF
2. **Chunking**: Text is split into overlapping chunks (800 chars, 200 overlap)
3. **Embedding**: Each chunk is converted to a 384-dimensional vector using MiniLM
4. **Indexing**: Vectors are stored in FAISS for fast similarity search
5. **Query Processing**: User question is embedded and similar chunks are retrieved
6. **Answer Generation**: Retrieved context + question is sent to Flan-T5 for answer generation

## ğŸ“Š API Endpoints

### `POST /upload`
Upload a PDF document
```json
{
  "file": "document.pdf"
}

### `POST /ask`
Ask a question
```json
{
  "question": "What is the refund policy?"
}

ğŸ“ What I Learned

- Implementing RAG (Retrieval-Augmented Generation) pipelines
- Vector similarity search with FAISS
- Semantic embeddings with transformer models
- Building REST APIs with FastAPI
- Deploying ML models in production
- Managing vector databases

ğŸ”® Future Improvements

- [ ] Support for multiple PDF uploads simultaneously
- [ ] Add conversation memory for follow-up questions
- [ ] Integrate better LLMs (GPT-4)
- [ ] Add citation tracking for answers
- [ ] Implement caching for faster responses
- [ ] Add authentication and user sessions


ğŸ‘¤ Author

Chavi Maru
GitHub: https://github.com/chavi19
LinkedIn: https://linkedin.com/in/yourprofile

---

â­ Star this repo if you found it helpful!
